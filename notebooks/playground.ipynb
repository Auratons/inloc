{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('/home/kremeto1/inloc/functions/inLocCIIRC_utils/projectMesh')\n",
    "import projectMesh\n",
    "sys.path.append('/home/kremeto1/hololens_mapper')\n",
    "from src.utils.UtilsMath import UtilsMath, renderDepth\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import open3d as o3d\n",
    "import scipy.io as sio\n",
    "import json\n",
    "from pathlib import Path\n",
    "from itertools import repeat\n",
    "import pandas as pd\n",
    "from pyntcloud import PyntCloud\n",
    "import cv2\n",
    "\n",
    "import pyrender\n",
    "import trimesh\n",
    "import numpy as np\n",
    "from pyrender.constants import RenderFlags\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "os.environ['NVIDIA_DRIVER_CAPABILITIES'] = 'compute,graphics,utility,video'\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = \"egl\"\n",
    "# When ran with SLURM on a multigpu node, scheduled on other than GPU0, we need\n",
    "# to set this or we get an egl initialization error.\n",
    "os.environ[\"EGL_DEVICE_ID\"] = os.environ.get(\"SLURM_JOB_GPUS\", \"0\").split(\",\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images, titles=None, cols=4, cmap=None, norm=None,\n",
    "                   interpolation=None):\n",
    "    \"\"\"Display the given set of images, optionally with titles.\n",
    "    images: list or array of image tensors in HWC format.\n",
    "    titles: optional. A list of titles to display with each image.\n",
    "    cols: number of images per row\n",
    "    cmap: Optional. Color map to use. For example, \"Blues\".\n",
    "    norm: Optional. A Normalize instance to map values to colors.\n",
    "    interpolation: Optional. Image interpolation to use for display.\n",
    "    \"\"\"\n",
    "    titles = titles if titles is not None else [\"\"] * len(images)\n",
    "    rows = len(images) // cols + 1\n",
    "    plt.figure(figsize=(28, 28 * rows // cols))\n",
    "    i = 1\n",
    "    for image, title in zip(images, titles):\n",
    "        plt.subplot(rows, cols, i)\n",
    "        plt.title(title, fontsize=9)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(image.astype(np.uint8), cmap=cmap,\n",
    "                   norm=norm, interpolation=interpolation)\n",
    "        i += 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splatting = 'pipeline-inloc-conv5-splatting'\n",
    "pyrender = 'pipeline-inloc-conv5-pyrender'\n",
    "lifted = 'pipeline-inloc-lifted-conv5-pyrender'\n",
    "\n",
    "prefix = '/home/kremeto1/inloc/datasets/'\n",
    "suffix = '/candidate_renders/IMG_0819/cutout_DUC_cutout_019_150_-30_color.png'\n",
    "\n",
    "for path in Path(prefix + splatting + \"/candidate_renders\").iterdir():\n",
    "    if path.is_dir():\n",
    "        print(path)\n",
    "        im = cv2.imread(f'/home/kremeto1/neural_rendering/datasets/raw/inloc/query/same_db_size_conditionally_rotated/{path.name}.JPG')\n",
    "        for img in path.glob(\"*_color.png\"):\n",
    "            images = [cv2.imread(str(path / img.name)) for i in [splatting, pyrender, lifted]] + [im]\n",
    "            display_images(images, cols=4)\n",
    "    \n",
    "\n",
    "# images = [cv2.imread(prefix + i + suffix) for i in [splatting, pyrender, lifted]]\n",
    "# display_images(images, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load('/home/kremeto1/neural_rendering/datasets/processed/inloc/inloc_rendered_splatting/CSE3/000/cse_cutout_000_0_0_depth.png.npy')\n",
    "d = np.clip(d * 255 / 100, 0, 255).astype(np.uint8)\n",
    "cv2.imwrite('/home/kremeto1/neural_rendering/datasets/processed/inloc/inloc_rendered_splatting/CSE3/000/cse_cutout_000_0_0_depth_uin8.png', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sio.loadmat('/home/kremeto1/inloc/datasets/hagia-pyrender/densePE_top100_shortlist.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = str(mat['ImgList'][0][0][0][0])\n",
    "candidate_paths = [str(i[0]) for i in mat['ImgList'][0][0][1][0]]\n",
    "candidate_scores = mat['ImgList'][0][0][2][0]\n",
    "candidate_transforms = [i for i in mat['ImgList'][0][0][3][0]]\n",
    "query_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_base = '/home/kremeto1/inloc/dvc/pipeline-hagia-pyrender/logs/inloc_algo_2022667_stats'\n",
    "# log_base = '/home/kremeto1/inloc/dvc/pipeline-hagia-conv5-pyrender/logs/inloc_algo_2022668_stats'\n",
    "log_base = '/home/kremeto1/inloc/dvc/pipeline-inloc-conv5-pyrender/logs/inloc_algo_2125436_stats'\n",
    "import re\n",
    "with open(f'{log_base}.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "new_lines = []\n",
    "for line in lines:\n",
    "    m = re.match('([0-9:]*)[ ]*PID S     TIME %CPU   RSS COMMAND', line)\n",
    "    if m is not None:\n",
    "        time = m.group(1)\n",
    "    m = re.match('.*MATLAB.*', line)\n",
    "    if m is not None:\n",
    "        line = f\"{time} {line.strip()}\"\n",
    "        line = re.sub('[ ]+', ' ', line)\n",
    "        line = ' '.join(line.split(' ')[:9]) + '\\n'\n",
    "        new_lines.append(line)\n",
    "\n",
    "with open(f'{log_base}_processed.txt', 'w') as f:\n",
    "    f.writelines(new_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = pd.read_csv(f'{log_base}_processed.txt', sep=' ', header=None, names=[f\"col{i}\" for i in range(9)])\n",
    "f = ff.groupby([\"col0\"]).sum()\n",
    "plt.figure(figsize=(18, 12))\n",
    "(ff.groupby([\"col0\"]).sum()[\"col5\"] / 1e6).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reverse_dict(d):\n",
    "    return {v: k for k, v in d.items()}\n",
    "\n",
    "input_mapping = '/nfs/projects/artwin/experiments/hololens_mapper/joined_dataset/mapping.txt'\n",
    "input_root = Path('/nfs/projects/artwin/experiments/hololens_mapper/joined_dataset/train')\n",
    "\n",
    "# Get mapping from reference images in joined dataset for nriw training to\n",
    "# reference images in a concrete nriw training dataset from which the joined\n",
    "# one was generated.\n",
    "with open(input_mapping, 'r') as f:\n",
    "    sub_mapping_1 = json.load(f)\n",
    "sub_mapping_1 = reverse_dict(sub_mapping_1)\n",
    "\n",
    "# Get roots of nriw training datasets from which a joined dataset was generated.\n",
    "source_roots = {}\n",
    "for path in [Path(k).parent.parent for k in sub_mapping_1.values()]:\n",
    "    source_roots[str(path)] = 1\n",
    "source_roots = list(source_roots.keys())\n",
    "\n",
    "# Get mapping from partial nriw training datasets to source references\n",
    "# generated by matlab from artwin panoramas\n",
    "sub_mappings_2 = []\n",
    "for mp in [Path(root) / \"mapping.txt\" for root in source_roots]:\n",
    "    with open(mp, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        # Filter lines only for used source (train/val/test)\n",
    "        lines = filter(lambda line: str(input_root.name).upper() in line, lines)\n",
    "        lines = [str.join(\" \", line.split(\" \")[:-1]) for line in lines]  # Get rid of trailing TRAIN/DEV/TEST\n",
    "        line_tuples = [tuple(line.split(\" -> \")) for line in lines]  # (source, dest)\n",
    "        sub_map = {}\n",
    "        for wut in zip(line_tuples):\n",
    "            v, k = wut[0]\n",
    "            sub_map[str(Path(mp).parent / str(input_root.name) / f\"{int(k):04n}_reference.png\")] = f\"/nfs/projects/artwin/experiments/as_colmap_60_fov_pyrender/{str.join('_', v.split('_')[0:2])}/images/{v}\"\n",
    "        sub_mappings_2.append(sub_map)\n",
    "\n",
    "# Get mapping from reference images in joined dataset for nriw training to\n",
    "# source references generated by matlab from artwin panoramas\n",
    "mapping = {}\n",
    "for k, v in sub_mapping_1.items():\n",
    "    for sm in sub_mappings_2:\n",
    "        if v in sm:\n",
    "            mapping[k] = sm[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str29 = '2019-09-28_08.31.29'\n",
    "str53 = '2019-09-28_16.11.53'\n",
    "ply29 = f'/nfs/projects/artwin/experiments/as_colmap_60_fov_pyrender/{str29}/{str29}_simplified.ply'\n",
    "ply53 = f'/nfs/projects/artwin/experiments/as_colmap_60_fov_pyrender/{str53}/{str53}_simplified.ply'\n",
    "f = 960/np.tan(30*np.pi/180)\n",
    "u0 = 1920/2\n",
    "v0 = 1080/2\n",
    "k = np.array([[f, 0, u0], [0, f, v0], [0, 0, 1]])\n",
    "img_size =  np.array([1920, 1080])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def o3d_to_pyrenderer(mesh_or_pt):\n",
    "    if isinstance(mesh_or_pt, o3d.geometry.PointCloud):\n",
    "        points = np.asarray(mesh_or_pt.points).copy()\n",
    "        colors = np.asarray(mesh_or_pt.colors).copy()\n",
    "        mesh = pyrender.Mesh.from_points(points, colors)\n",
    "    elif isinstance(mesh_or_pt, o3d.geometry.TriangleMesh):\n",
    "        mesh = trimesh.Trimesh(\n",
    "            np.asarray(mesh_or_pt.vertices),\n",
    "            np.asarray(mesh_or_pt.triangles),\n",
    "            vertex_colors=np.asarray(mesh_or_pt.vertex_colors),\n",
    "        )\n",
    "        mesh = pyrender.Mesh.from_trimesh(mesh)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    return mesh\n",
    "\n",
    "def to_o3d(rgb, xyz):\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(xyz.copy().astype(np.float64).T)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(rgb.copy().astype(np.float64).T)\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = sio.loadmat('/home/kremeto1/inloc/datasets/artwin-small-set/densePE_top100_shortlist.mat')\n",
    "query_images = pos['ImgList'][0][0]\n",
    "sorted_db_images = pos['ImgList'][0][0][1]\n",
    "sorted_db_scores = pos['ImgList'][0][0][2]\n",
    "sorted_matrices = pos['ImgList'][0][0][3]\n",
    "\n",
    "# Loading the mesh / pointcloud\n",
    "# mesh = load_ply(ply_path, voxel_size)\n",
    "pcds = {}; rgbs = {}; scenes = {}; xyzs = {}\n",
    "\n",
    "for str_ in [str29, str53]:\n",
    "    pcd = o3d.io.read_point_cloud(\n",
    "        f'/nfs/projects/artwin/experiments/as_colmap_60_fov_pyrender/{str_}/{str_}_simplified.ply',\n",
    "        remove_nan_points=True, remove_infinite_points=True, print_progress=True\n",
    "    )\n",
    "    rgb = np.asarray(pcd.colors).T\n",
    "    rgb *= 255\n",
    "    rgb = rgb.astype(np.uint8)\n",
    "    xyz = np.asarray(pcd.points).T\n",
    "    pcds[str_] = pcd\n",
    "    rgbs[str_] = rgb\n",
    "    xyzs[str_] = xyz\n",
    "    # For artwin all images have the same dimensions, pre-creating EGL context\n",
    "    # within renderer speeds up the rendering loop.\n",
    "    scenes[str_] = pyrender.Scene(bg_color=[0.0,0.0,0.0])\n",
    "    scenes[str_].add(o3d_to_pyrenderer(pcd))\n",
    "\n",
    "utils_math = UtilsMath()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(query_images[0][0])\n",
    "mapping[query_images[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3d.visualization.draw_geometries([pcds[str29]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rndr = pyrender.OffscreenRenderer(1920, 1080, point_size=15)\n",
    "flags = RenderFlags.FLAT | RenderFlags.RGBA | RenderFlags.DEPTH_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_T(T):\n",
    "    \"\"\"Invert a 4x4 transformation matrix.\"\"\"\n",
    "    T_inv = np.eye(4)\n",
    "    r, t = T[:3, :3], T[:3, -1]\n",
    "    T_inv[:3, :3] = r.T\n",
    "    T_inv[:3, -1] = -r.T @ t\n",
    "    return T_inv\n",
    "\n",
    "ROT_ALIGN_QUERY = np.array([\n",
    "    [1, 0, 0],\n",
    "    [0, -1, 0],\n",
    "    [0, 0, -1]\n",
    "], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%env DISPLAY=localhost:10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2h(pts):\n",
    "    \"\"\"Affine to homogeneous coordinates, (3, N) -> (4, N) shape.\"\"\"\n",
    "    return np.vstack((pts, np.ones((1, pts.shape[1]))))\n",
    "\n",
    "def h2a(pts):\n",
    "    \"\"\"Homogeneous to affine coordinates, (4, N) -> (3, N) shape.\"\"\"\n",
    "    return (pts / pts[3,:])[:3,:]\n",
    "\n",
    "def clip_view_frustrum(xyz, rgb, affine_transform):\n",
    "    \"\"\"Clip points and colors to view frustrum (without znear and zfar clipping).\"\"\"\n",
    "    pairs = np.array([[0, 1, 2, 3], [1, 2, 3, 0]])\n",
    "    pts_cam = np.array([[-1, -1, 1], [1, -1, 1], [1, 1, 1], [-1, 1, 1]]).T\n",
    "    at = affine_transform.copy()\n",
    "    pts_world = h2a(at @ a2h(pts_cam))\n",
    "    t = affine_transform[:3, -1:]\n",
    "    \n",
    "    at[:, 1:3] *= -1\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pts = np.zeros((5 + 4 + 1000, 3), dtype=np.float64)\n",
    "    pts[:4, :] = pts_world.astype(np.float64).T\n",
    "    pts[4:8, :] = h2a(at @ a2h(pts_cam)).astype(np.float64).T\n",
    "    pts[8, :] = t.squeeze().astype(np.float64).T\n",
    "    pts[9:, :] = xyz[:,:1000].copy().astype(np.float64).T\n",
    "    pcd.points = o3d.utility.Vector3dVector(pts)\n",
    "    colors = np.array([[1,0,0], [1,0,0], [1,0,0], [1,0,0], [1,0,0], [1,0,0], [1,0,0], [1,0,0], [0,1,0]] + list(repeat([0,0,1], 1000)), dtype=np.float64)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    print(np.asarray(pcd.points).shape)\n",
    "    print(np.asarray(pcd.points).dtype)\n",
    "    print(np.asarray(pcd.points).data.contiguous)\n",
    "    # print(np.asarray(pcd.colors).shape)\n",
    "    # print(np.asarray(pcd.colors).dtype)\n",
    "    # print(np.asarray(pcd.colors).data.contiguous)\n",
    "    \n",
    "    # o3d.visualization.draw_geometries([pcd])\n",
    "    o3d.io.write_point_cloud(\"sync.ply\", pcd)\n",
    "    \n",
    "    # cloud = PyntCloud(pd.DataFrame(\n",
    "    # # same arguments that you are passing to visualize_pcl\n",
    "    # data=np.hstack((pts, colors)),\n",
    "    # columns=[\"x\", \"y\", \"z\", \"red\", \"green\", \"blue\"]))\n",
    "\n",
    "    # cloud.to_file(\"output.ply\")\n",
    "\n",
    "    Ndpts = xyz.shape[1]\n",
    "    vpts = xyz - t\n",
    "    D = np.ones(Ndpts, dtype=bool)\n",
    "    t = t.squeeze()\n",
    "    for j in range(0, 4):\n",
    "        v2 = pts_world[:, pairs[0, j]] - t\n",
    "        v1 = pts_world[:, pairs[1, j]] - t\n",
    "        D = D & (((v2[0] * v1[1]) * vpts[2, :] - (v2[2] * v1[1]) * vpts[0, :] +\n",
    "                (v2[2] * v1[0]) * vpts[1, :] - (v2[0] * v1[2]) * vpts[1, :] +\n",
    "                (v2[1] * v1[2]) * vpts[0, :] - (v2[1] * v1[0]) * vpts[2, :]) > 0)\n",
    "    return xyz[:, D], rgb[:, D]\n",
    "\n",
    "def render_colmap_image(data, max_radius=5):\n",
    "    \"\"\"Render image so that it agrees with artwin reference images.\"\"\"\n",
    "    K   = data[\"K\"]\n",
    "    R   = data[\"R\"]\n",
    "    C   = data[\"C\"]\n",
    "    h   = data[\"h\"]\n",
    "    w   = data[\"w\"]\n",
    "    xyz = data[\"xyz\"]\n",
    "    rgb = data[\"rgb\"]\n",
    "\n",
    "    # project points\n",
    "    camera2world = np.eye(4)\n",
    "    camera2world[:3, :3] = R.T\n",
    "    camera2world[:3, -1:] = C\n",
    "    camera2world[:, 1:3] *= -1\n",
    "    \n",
    "    xyz, rgb = clip_view_frustrum(xyz.copy(), rgb.copy(), camera2world)\n",
    "\n",
    "    xyz_camera_homogeneous = np.linalg.inv(camera2world) @ a2h(xyz)\n",
    "    xyz_camera = h2a(xyz_camera_homogeneous)\n",
    "    depth = np.linalg.norm(xyz_camera, axis=0)\n",
    "\n",
    "    # focal_length = (float(K[0, 0]) + float(K[1, 1])) / 2\n",
    "    # df = depth > focal_length\n",
    "    # print(max(depth))\n",
    "    # print(min(depth))\n",
    "    # depth = depth[df]\n",
    "    # xyz_camera = xyz_camera[:, df]\n",
    "\n",
    "    uv_homogeneous = K @ xyz_camera\n",
    "    uv = (uv_homogeneous / uv_homogeneous[2, :])[:2, :]\n",
    "    u = uv[0, :]\n",
    "    v = uv[1, :]\n",
    "\n",
    "    # get data in image\n",
    "    filtering = np.array((u >= 0) & (v >= 0) & (u < w) & (v < h) & (uv_homogeneous[2, :] < 0)).squeeze()\n",
    "    depth_filtered = depth[filtering.squeeze()]\n",
    "    # sort according to depth\n",
    "    ordering = np.flip(np.argsort(depth_filtered))\n",
    "\n",
    "    depth = depth_filtered[ordering.squeeze()]\n",
    "    uv = uv[:, filtering][:, ordering]\n",
    "    rgb = rgb[:, filtering][:, ordering]\n",
    "\n",
    "    # Linear interpolation between points (min(depth), min_radius) and (max(depth), max_radius),\n",
    "    # coefficients computation\n",
    "    min_radius = 5\n",
    "    k_ = (max_radius - min_radius) / (max(depth) - min(depth))\n",
    "    q_ = min_radius - k_ * min(depth)\n",
    "    # Linear interpolation itself + ensuring only odd radii are present (C++ impl requirement)\n",
    "    radii = np.round((k_ * depth + q_ + 1) / 2).astype(np.uint16)\n",
    "\n",
    "    # run cpp to render visibility information\n",
    "    uv[0,:] = w - uv[0,:]  # Flip so that it agrees with reference images from artwin\n",
    "    img = renderDepth.render_image(h, w, uv, radii, rgb)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0,1):\n",
    "    T_query = np.concatenate([sorted_matrices[0][idx], np.array([[0, 0, 0, 1]])], axis=0)\n",
    "    T_query_inv = invert_T(T_query.copy())\n",
    "\n",
    "    # camera_pose = T_query_inv.copy()\n",
    "    # r = camera_pose[:3, :3].copy()  # sorted_matrices[0][idx][:3, :3]\n",
    "    # camera_pose[:, 1:3] *= -1\n",
    "    # t = camera_pose[:3, -1:]  # sorted_matrices[0][idx][:3, 3].reshape((3, 1))\n",
    "    # r = np.linalg.inv(k) @ r\n",
    "    # t = np.linalg.inv(k) @ t\n",
    "\n",
    "    dbpath = sorted_db_images[0][idx][0]\n",
    "    if str29 in dbpath:\n",
    "        rgb = rgbs[str29]\n",
    "        xyz = xyzs[str29]\n",
    "        scene = scenes[str29]\n",
    "    else:\n",
    "        rgb = rgbs[str53]\n",
    "        xyz = xyzs[str53]\n",
    "        scene = scenes[str53]\n",
    "\n",
    "    camera = pyrender.IntrinsicsCamera(\n",
    "        k[0, 0], k[1, 1], k[0, 2], k[1, 2]\n",
    "    )\n",
    "    camera_pose = T_query_inv.copy()\n",
    "    # camera_pose[:3, :3] = r\n",
    "    # camera_pose[:3, -1:] = -r @ t\n",
    "    camera_pose[:, 1:3] *= -1\n",
    "    camera_node = pyrender.Node(camera=camera, matrix=camera_pose)\n",
    "    scene.add_node(camera_node)\n",
    "\n",
    "    # Offscreen rendering\n",
    "    K = k\n",
    "    R = r\n",
    "    C = t\n",
    "    h = 1080\n",
    "    w = 1920\n",
    "    rgb = rgb\n",
    "    xyz = xyz\n",
    "    max_radius=18\n",
    "    \n",
    "    # project points\n",
    "    camera2world = np.eye(4)\n",
    "    camera2world[:3, :3] = R.T\n",
    "    camera2world[:3, -1:] = C\n",
    "    camera2world[:, 1:3] *= -1\n",
    "    \n",
    "    xyz, rgb = clip_view_frustrum(xyz.copy(), rgb.copy(), T_query_inv)\n",
    "    o3d.io.write_point_cloud(\"sync.ply\", to_o3d(rgb, xyz))\n",
    "    # o3d.visualization.draw_geometries([to_o3d(rgb, xyz)])\n",
    "\n",
    "    xyz_camera_homogeneous = np.linalg.inv(camera2world) @ a2h(xyz)\n",
    "    xyz_camera = h2a(xyz_camera_homogeneous)\n",
    "    depth = np.linalg.norm(xyz_camera, axis=0)\n",
    "\n",
    "    # focal_length = (float(K[0, 0]) + float(K[1, 1])) / 2\n",
    "    # df = depth > focal_length\n",
    "    # print(max(depth))\n",
    "    # print(min(depth))\n",
    "    # depth = depth[df]\n",
    "    # xyz_camera = xyz_camera[:, df]\n",
    "\n",
    "    uv_homogeneous = K @ xyz_camera\n",
    "    uv = (uv_homogeneous / uv_homogeneous[2, :])[:2, :]\n",
    "    u = uv[0, :]\n",
    "    v = uv[1, :]\n",
    "\n",
    "    # get data in image\n",
    "    filtering = np.array((u >= 0) & (v >= 0) & (u < w) & (v < h) & (uv_homogeneous[2, :] < 0)).squeeze()\n",
    "    depth_filtered = depth[filtering.squeeze()]\n",
    "    # sort according to depth\n",
    "    ordering = np.flip(np.argsort(depth_filtered))\n",
    "\n",
    "    depth = depth_filtered[ordering.squeeze()]\n",
    "    uv = uv[:, filtering][:, ordering]\n",
    "    rgb = rgb[:, filtering][:, ordering]\n",
    "\n",
    "    # Linear interpolation between points (min(depth), min_radius) and (max(depth), max_radius),\n",
    "    # coefficients computation\n",
    "    min_radius = 5\n",
    "    k_ = (max_radius - min_radius) / (max(depth) - min(depth))\n",
    "    q_ = min_radius - k_ * min(depth)\n",
    "    # Linear interpolation itself + ensuring only odd radii are present (C++ impl requirement)\n",
    "    radii = np.round((k_ * depth + q_ + 1) / 2).astype(np.uint16)\n",
    "\n",
    "    # run cpp to render visibility information\n",
    "    uv[0,:] = w - uv[0,:]  # Flip so that it agrees with reference images from artwin\n",
    "    rgb_rendering = renderDepth.render_image(h, w, uv, radii, rgb)\n",
    "\n",
    "    depth_rendering = rndr.render(scene, flags=flags)\n",
    "    scene.remove_node(camera_node)\n",
    "    print(f\"Processed {dbpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(rgb_rendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/nfs/projects/artwin/experiments/as_colmap_60_fov_pyrender/2019-09-28_08.31.29/images/2019-09-28_08.31.29_00004_x30_z240_reference.png')\n",
    "# query = '/nfs/projects/artwin/experiments/hololens_mapper/joined_dataset/test/0000_reference.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_central_crop(img, crop_height=512, crop_width=512):\n",
    "    if len(img.shape) == 2:\n",
    "        img = np.expand_dims(img, axis=2)\n",
    "    assert len(img.shape) == 3, (\n",
    "        \"input image should be either a 2D or 3D matrix,\"\n",
    "        \" but input was of shape %s\" % str(img.shape)\n",
    "    )\n",
    "    height, width, ch = img.shape\n",
    "    # assert height >= crop_height and width >= crop_width, (\n",
    "    #     \"input image cannot \" \"be smaller than the requested crop size\"\n",
    "    # )\n",
    "    if height >= crop_height and width >= crop_width:\n",
    "        st_y = (height - crop_height) // 2\n",
    "        st_x = (width - crop_width) // 2\n",
    "        return np.squeeze(img[st_y : st_y + crop_height, st_x : st_x + crop_width, :])\n",
    "    else:\n",
    "        new_img = np.zeros((crop_height, crop_height, ch), dtype=np.float32)\n",
    "        new_img[st_y : st_y + height, st_x : st_x + width, :] = img\n",
    "        return np.squeeze(new_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(Image.open('/home/kremeto1/neural_rendering/datasets/raw/inloc/query/iphone7/IMG_0938.JPG')).astype(np.float32)/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rendered_img = np.array(Image.open('/home/kremeto1/neural_rendering/datasets/raw/inloc/query/iphone7/IMG_0938.JPG')).astype(np.float32)\n",
    "print(rendered_img.shape)\n",
    "rendered_img = get_central_crop(rendered_img, 1600, 1600)\n",
    "print(rendered_img.shape)\n",
    "plt.imshow(rendered_img/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ply(ply_path, voxel_size):\n",
    "    # Loading the mesh / pointcloud\n",
    "    m = trimesh.load(ply_path)\n",
    "    if isinstance(m, trimesh.PointCloud):\n",
    "        if voxel_size is not None:\n",
    "            pcd = o3d.geometry.PointCloud()\n",
    "            pcd.points = o3d.utility.Vector3dVector(np.asarray(m.vertices))\n",
    "            pcd.colors = o3d.utility.Vector3dVector(\n",
    "                np.asarray(m.colors, dtype=np.float64)[:, :3] / 255\n",
    "            )\n",
    "            pcd = pcd.voxel_down_sample(voxel_size=voxel_size)\n",
    "            mesh = o3d_to_pyrenderer(pcd)\n",
    "        else:\n",
    "            points = m.vertices.copy()\n",
    "            colors = m.colors.copy()\n",
    "            mesh = pyrender.Mesh.from_points(points, colors)\n",
    "    elif isinstance(m, trimesh.Trimesh):\n",
    "        if voxel_size is not None:\n",
    "            m2 = m.as_open3d\n",
    "            m2.vertex_colors = o3d.utility.Vector3dVector(\n",
    "                np.asarray(m.visual.vertex_colors, dtype=np.float64)[:, :3] / 255\n",
    "            )\n",
    "            m2 = m2.simplify_vertex_clustering(\n",
    "                voxel_size=voxel_size,\n",
    "                contraction=o3d.geometry.SimplificationContraction.Average,\n",
    "            )\n",
    "            mesh = o3d_to_pyrenderer(m2)\n",
    "        else:\n",
    "            mesh = pyrender.Mesh.from_trimesh(m)\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Unsupported 3D object. Supported format is a `.ply` pointcloud or mesh.\"\n",
    "        )\n",
    "    return mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /nfs/projects/artwin/experiments/as_colmap_60_fov_pyrender/2019-09-28_08.31.29/images/2019-09-28_08.31.29_00002_x0_z60_params.json\n",
    "data = {\n",
    "    \"x_rot_mat\": [[1,0,0],[0,6.123233995736766E-17,-1],[0,1,6.123233995736766E-17]],\n",
    "    \"z_rot_mat\": [[0.50000000000000011,-0.8660254037844386,0],[0.8660254037844386,0.50000000000000011,0],[0,0,1]],\n",
    "    \"pano_rot_mat\": [[-0.99218108866967136,-0.1246819992809647,-0.0055755126729838166],[0.12466066620694956,-0.99219127447899413,0.0040240711460282075],[-0.0060337042606172378,0.0032975601662910271,0.99997635997549683]],\n",
    "    \"pano_translation\": [-0.237677,-5.618579,1.862724],\n",
    "    \"calibration_mat\": [[1662.7687752661222,0,960],[0,1662.7687752661222,540],[0,0,1]],\n",
    "    \"source_pano_path\": \"/nfs/projects/artwin/SIE factory datasets/proc/2019-09-28_08.31.29/pano/00002-pano.jpg\",\"source_ply_path\":\"/nfs/projects/artwin/SIE factory datasets/proc/2019-09-28_08.31.29/2019-09-28_08.31.29_txt.ply\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_rot = np.array(data[\"x_rot_mat\"]) @ np.array(data[\"z_rot_mat\"]) @ np.array(data[\"pano_rot_mat\"]).T\n",
    "pcd_t = np.array(data[\"pano_translation\"])\n",
    "k = np.array(data[\"calibration_mat\"])\n",
    "f = 960/np.tan(30*np.pi/180)\n",
    "t = -pcd_rot.T @pcd_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = load_ply('/nfs/projects/artwin/experiments/as_colmap_60_fov_pyrender/2019-09-28_08.31.29/2019-09-28_08.31.29.ply', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = RenderFlags.FLAT | RenderFlags.RGBA\n",
    "scene = pyrender.Scene(bg_color=[0,0,0])\n",
    "scene.add(mesh)\n",
    "camera = pyrender.camera.IntrinsicsCamera(\n",
    "    k[0, 0], k[1, 1], k[0, 2], k[1, 2]\n",
    ")\n",
    "camera_pose = np.eye(4)\n",
    "camera_pose[:3, :3] = pcd_rot.T\n",
    "camera_pose[:3, -1:] = pcd_t.reshape((3,1))\n",
    "camera_pose[:, 1:3] *= -1\n",
    "scene.add(camera, pose=camera_pose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offscreen rendering\n",
    "r = pyrender.OffscreenRenderer(1920, 1080,point_size=5)\n",
    "rgb_rendering, depth_rendering = r.render(scene, flags=flags)\n",
    "r.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(rgb_rendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = projectMesh.projectMeshDebug(\n",
    "    '/nfs/projects/artwin/experiments/as_colmap_60_fov_pyrender/2019-09-28_08.31.29/2019-09-28_08.31.29_mesh.ply',\n",
    "    f,\n",
    "    pcd_rot,\n",
    "    t,\n",
    "    np.array([1920, 1080]),\n",
    "    False, None, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(x[1].sum(axis=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCentered3Dindices(mode, sensorWidth, sensorHeight):\n",
    "    if mode == 'x':\n",
    "        length = sensorWidth\n",
    "    elif mode == 'y':\n",
    "        length = sensorHeight\n",
    "    else:\n",
    "        raise ValueError('Unknown mode!')\n",
    "\n",
    "    halfFloat = length/2\n",
    "    halfInt = int(halfFloat)\n",
    "    lower = -halfInt\n",
    "    upper = halfInt\n",
    "    if not np.allclose(halfFloat, halfInt):\n",
    "        upper += 1\n",
    "    ind = np.arange(lower, upper)\n",
    "    if mode == 'x':\n",
    "        ind = np.broadcast_to(ind, (sensorHeight, sensorWidth)).T\n",
    "    elif mode == 'y':\n",
    "        ind = np.broadcast_to(ind, (sensorWidth, sensorHeight))\n",
    "    ind = np.reshape(ind, (sensorHeight*sensorWidth, 1))\n",
    "    ind = np.tile(ind, (1,3))\n",
    "    return ind\n",
    "    \n",
    "def buildXYZcut(sensorWidth, sensorHeight, t, cameraDirection, scaling, sensorXAxis, sensorYAxis, depth):\n",
    "    # TODO: compute xs, ys only once (may not actually matter)\n",
    "    ts = np.broadcast_to(t, (sensorHeight*sensorWidth, 3))\n",
    "    camDirs = np.broadcast_to(cameraDirection, (sensorHeight*sensorWidth, 3))\n",
    "    xs = getCentered3Dindices('x', sensorWidth, sensorHeight)\n",
    "    ys = getCentered3Dindices('y', sensorWidth, sensorHeight)\n",
    "    sensorXAxes = np.broadcast_to(sensorXAxis, (sensorHeight*sensorWidth, 3))\n",
    "    sensorYAxes = np.broadcast_to(sensorYAxis, (sensorHeight*sensorWidth, 3))\n",
    "    sensorDirs = camDirs + scaling * np.multiply(xs, sensorXAxes) + scaling * np.multiply(ys, sensorYAxes)\n",
    "    depths = np.reshape(depth.T, (sensorHeight*sensorWidth, 1))\n",
    "    depths = np.tile(depths, (1,3))\n",
    "    pts = ts + np.multiply(sensorDirs, depths)\n",
    "    xyzCut = np.reshape(pts, (sensorHeight, sensorWidth, 3))\n",
    "    return xyzCut, pts\n",
    "\n",
    "#     pixel_centers_in_screen_space = np.transpose(np.mgrid[-hh:hh, -hw:hw], (1, 2, 0))\n",
    "#     points_in_view_space = np.append(pixel_centers_in_screen_space / focal_length, np.ones(depth.shape + (1,)), axis=2) * np.repeat(depth.reshape(depth.shape + (1,)), 3, axis=2)\n",
    "#     points_in_view_space = points_in_view_space.reshape((-1, 3))\n",
    "#     points_in_world_space = (camera_pose @ np.concatenate([points_in_view_space, np.ones((points_in_view_space.shape[0], 1))], axis=1).T).T\n",
    "#     points_in_world_space = points_in_world_space[:, :3]\n",
    "#     points_in_world_space = points_in_world_space.reshape(pixel_centers_in_screen_space.shape[:2] + (3,))\n",
    "#     points_in_world_space.shape\n",
    "\n",
    "def project_mesh_core(depth, k, R, t, debug):\n",
    "    # camera_pose_rendering_convention = np.eye(4)\n",
    "    # camera_pose_rendering_convention[:3, :3] = R.T\n",
    "    # camera_pose_rendering_convention[:3, -1:] = -R.T @ t\n",
    "    # camera_pose_rendering_convention[:, 1:3] *= -1  # This is change of convention\n",
    "    \n",
    "    \n",
    "    \n",
    "    sensor_width = depth.shape[1]\n",
    "    sensor_height = depth.shape[0]\n",
    "    assert k[0, 0] == k[1, 1], \"Camera pixel is not square.\"\n",
    "    focal_length = k[0, 0]\n",
    "    scaling = 1.0 / focal_length\n",
    "    space_coord_system = np.eye(3)\n",
    "    sensor_coord_system = np.matmul(R, space_coord_system)\n",
    "    sensor_x_axis = sensor_coord_system[:, 0]\n",
    "    sensor_y_axis = sensor_coord_system[:, 1]\n",
    "    # make camera point toward -z by default, as in OpenGL\n",
    "    camera_dir = sensor_coord_system[:, 2] # unit vector\n",
    "\n",
    "    xyz_cut, pts = buildXYZcut(\n",
    "        sensor_width, sensor_height,\n",
    "        t, camera_dir, scaling,\n",
    "        sensor_x_axis, sensor_y_axis, depth\n",
    "    )\n",
    "\n",
    "    xyz_pc = -1\n",
    "    if debug:\n",
    "        xyz_pc = o3d.geometry.PointCloud()\n",
    "        xyz_pc.points = o3d.utility.Vector3dVector(pts)\n",
    "\n",
    "    return xyz_cut, pts\n",
    "\n",
    "def squarify(image: np.array, square_size: int) -> np.array:\n",
    "    assert square_size >= image.shape[0] and square_size >= image.shape[1]\n",
    "    shape = list(image.shape)\n",
    "    shape[0] = shape[1] = square_size\n",
    "    square = np.zeros(shape, dtype=image.dtype)\n",
    "    h, w = image.shape[:2]\n",
    "    offset_h = (square_size - h) // 2\n",
    "    offset_w = (square_size - w) // 2\n",
    "    square[offset_h : offset_h + h, offset_w : offset_w + w, ...] = image\n",
    "    return square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/kremeto1/inloc/inLocCIIRC_dataset/buildCutouts')\n",
    "import read_model\n",
    "\n",
    "def get_colmap_file(colmap_path, file_stem):\n",
    "    colmap_path = Path(colmap_path)\n",
    "    fp = colmap_path / f\"{file_stem}.bin\"\n",
    "    if not fp.exists():\n",
    "        fp = colmap_path / f\"{file_stem}.txt\"\n",
    "    return str(fp)\n",
    "\n",
    "# Load camera matrices and names of corresponding src images from\n",
    "# colmap images.bin and cameras.bin files from colmap sparse reconstruction\n",
    "def load_cameras_colmap(images_fp, cameras_fp):\n",
    "    if images_fp.endswith(\".bin\"):\n",
    "        images = read_model.read_images_binary(images_fp)\n",
    "    else:  # .txt\n",
    "        images = read_model.read_images_text(images_fp)\n",
    "\n",
    "    if cameras_fp.endswith(\".bin\"):\n",
    "        cameras = read_model.read_cameras_binary(cameras_fp)\n",
    "    else:  # .txt\n",
    "        cameras = read_model.read_cameras_text(cameras_fp)\n",
    "\n",
    "    src_img_nms = []\n",
    "    K = []\n",
    "    T = []\n",
    "    R = []\n",
    "    w = []\n",
    "    h = []\n",
    "\n",
    "    for i in images.keys():\n",
    "        R.append(read_model.qvec2rotmat(images[i].qvec))\n",
    "        T.append((images[i].tvec)[..., None])\n",
    "        k = np.eye(3)\n",
    "        camera = cameras[images[i].camera_id]\n",
    "        if camera.model in [\"SIMPLE_RADIAL\", \"SIMPLE_PINHOLE\"]:\n",
    "            k[0, 0] = cameras[images[i].camera_id].params[0]\n",
    "            k[1, 1] = cameras[images[i].camera_id].params[0]\n",
    "            k[0, 2] = cameras[images[i].camera_id].params[1]\n",
    "            k[1, 2] = cameras[images[i].camera_id].params[2]\n",
    "        elif camera.model in [\"RADIAL\", \"PINHOLE\"]:\n",
    "            k[0, 0] = cameras[images[i].camera_id].params[0]\n",
    "            k[1, 1] = cameras[images[i].camera_id].params[1]\n",
    "            k[0, 2] = cameras[images[i].camera_id].params[2]\n",
    "            k[1, 2] = cameras[images[i].camera_id].params[3]\n",
    "        # TODO : Take other camera models into account + factorize\n",
    "        else:\n",
    "            raise NotImplementedError(\"Camera models not supported yet!\")\n",
    "\n",
    "        K.append(k)\n",
    "        w.append(cameras[images[i].camera_id].width)\n",
    "        h.append(cameras[images[i].camera_id].height)\n",
    "        src_img_nms.append(images[i].name)\n",
    "\n",
    "    return K, R, T, h, w, src_img_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "args = {\n",
    "  \"input_root\": Path(\"/home/kremeto1/neural_rendering/datasets/post_processed/imc/hagia_sophia_interior_thesis_test-100_src-fused\"),\n",
    "  \"input_ply_path\": Path(\"/home/kremeto1/neural_rendering/datasets/processed/imc/hagia_sophia_interior/dense/dense/0/fused.ply\"),\n",
    "  \"input_root_colmap\": Path(\"/home/kremeto1/neural_rendering/datasets/processed/imc/hagia_sophia_interior/dense/dense/0/sparse\"),\n",
    "  \"output_root\": Path(\"/home/kremeto1/neural_rendering/datasets/final/imc/hagia_sophia_interior_thesis_test_squarify-100_src-fused_inloc_format\"),\n",
    "  \"test_size\": 50,\n",
    "  \"squarify\": True\n",
    "}\n",
    "# args = {\n",
    "#   \"input_root\": Path(\"/nfs/projects/artwin/experiments/thesis/artwin_as_inloc/2019-09-28_08.31.29-splatting\"),\n",
    "#   \"input_ply_path\": Path(\"/nfs/projects/artwin/experiments/thesis/artwin_as_inloc/2019-09-28_08.31.29/2019-09-28_08.31.29.ply\"),\n",
    "#   \"input_root_colmap\": Path(\"/nfs/projects/artwin/experiments/thesis/artwin_as_inloc/2019-09-28_08.31.29/sparse\"),\n",
    "#   \"output_root\": Path(\"/nfs/projects/artwin/experiments/thesis/artwin_as_inloc/2019-09-28_08.31.29-splatting-inloc_format\"),\n",
    "#   \"test_size\": \"0\",\n",
    "#   \"val_ratio\": \"1.0\",\n",
    "#   \"squarify\": \"False\"\n",
    "# }\n",
    "args = argparse.Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks, Rs, Ts, Hs, Ws, img_nms = load_cameras_colmap(\n",
    "    get_colmap_file(args.input_root_colmap, \"images\"),\n",
    "    get_colmap_file(args.input_root_colmap, \"cameras\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "indices = list(range(len(Hs)))\n",
    "\n",
    "test_idx = 584\n",
    "test_idx = 50\n",
    "photo_path = args.input_root / \"train\" / \"{:04n}_reference.png\".format(test_idx)\n",
    "if not photo_path.exists():\n",
    "    photo_path = args.input_root / \"test\" / \"{:04n}_reference.png\".format(test_idx)\n",
    "rnd = random.Random(42)\n",
    "rnd.shuffle(indices)\n",
    "print(indices[test_idx])\n",
    "\n",
    "calibration_mat = Ks[indices[test_idx]]\n",
    "rotation_mat = Rs[indices[test_idx]]\n",
    "translation = Ts[indices[test_idx]]\n",
    "output_root = args.output_root\n",
    "square = args.squarify\n",
    "\n",
    "stem = photo_path.stem.strip(\"_reference\")\n",
    "\n",
    "depth_npy = photo_path.parent / (stem + \"_depth.npy\")\n",
    "if not depth_npy.exists():\n",
    "    depth_npy = photo_path.parent / (stem + \"_depth.png.npy\")\n",
    "mesh_projection = photo_path.parent / (stem + \"_color.png\")\n",
    "cutout_reference = photo_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = calibration_mat\n",
    "hh = k[1, 2]\n",
    "hw = k[0, 2]\n",
    "assert k[0, 0] == k[1, 1], \"Camera pixel is not square.\"\n",
    "focal_length = k[0, 0]\n",
    "\n",
    "dataset_depth = np.load(str(depth_npy))\n",
    "# dataset_depth = dataset_depth * 100 / 255\n",
    "# dataset_depth = np.divide(dataset_depth, np.sqrt(np.square(np.transpose(np.mgrid[-hh:hh, -hw:hw], (1, 2, 0)) / focal_length).sum(axis=2) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "prj = plt.imread(str(mesh_projection), cv2.IMREAD_UNCHANGED)[:, :, :3]\n",
    "ref = plt.imread(str(cutout_reference), cv2.IMREAD_UNCHANGED)[:, :, :3]\n",
    "# dataset_depth = np.load(str(depth_npy))\n",
    "# XYZcut, pts = project_mesh_core(dataset_depth, calibration_mat, rotation_mat.T, (- rotation_mat.T @ translation).squeeze(), False)\n",
    "XYZcut, pts = project_mesh_core(dataset_depth, calibration_mat, rotation_mat.T, (- rotation_mat.T @ translation).squeeze(), False)\n",
    "# if square:\n",
    "#     XYZcut = squarify(XYZcut, ref.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = np.broadcast_to(t, (sensorHeight*sensorWidth, 3))\n",
    "camDirs = np.broadcast_to(cameraDirection, (sensorHeight*sensorWidth, 3))\n",
    "xs = getCentered3Dindices('x', sensorWidth, sensorHeight)\n",
    "ys = getCentered3Dindices('y', sensorWidth, sensorHeight)\n",
    "sensorXAxes = np.broadcast_to(sensorXAxis, (sensorHeight*sensorWidth, 3))\n",
    "sensorYAxes = np.broadcast_to(sensorYAxis, (sensorHeight*sensorWidth, 3))\n",
    "sensorDirs = camDirs + scaling * np.multiply(xs, sensorXAxes) + scaling * np.multiply(ys, sensorYAxes)\n",
    "depths = np.reshape(depth.T, (sensorHeight*sensorWidth, 1))\n",
    "depths = np.tile(depths, (1,3))\n",
    "pts = ts + np.multiply(sensorDirs, depths)\n",
    "xyzCut = np.reshape(pts, (sensorHeight, sensorWidth, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS IS FUCKING IMPORTANT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_centers = np.append(np.transpose(np.mgrid[-hh:hh, -hw:hw], (1, 2, 0)), focal_length*np.ones((766, 1025) + (1,)), axis=2)\n",
    "# pixel_centers*=-1\n",
    "# mm = np.linalg.inv(calibration_mat)\n",
    "# rays = np.matmul(np.tile(mm, (766, 1025, 1, 1)), pixel_centers[:, :, :, np.newaxis]).squeeze()\n",
    "rays = pixel_centers / np.linalg.norm(pixel_centers, axis=2)[:,:,np.newaxis]\n",
    "\n",
    "cam_pose = np.eye(4)\n",
    "cam_pose[:3, :3] = rotation_mat.T\n",
    "cam_pose[:3, -1:] = (- rotation_mat.T @ translation)\n",
    "cam_pose[:3, :3] = cam_pose[:3, :3] @ R.from_euler('z', 90, degrees=True).as_matrix()\n",
    "# cam_pose[:, 1:3] *= -1\n",
    "\n",
    "points = np.multiply(rays, np.repeat(dataset_depth[:, ::-1, np.newaxis], 3, axis=2))\n",
    "points = np.append(points, np.ones((766, 1025) + (1,)), axis=2)\n",
    "points = np.matmul(np.tile(cam_pose, (766, 1025, 1, 1)), points[:, :, :, np.newaxis]).squeeze()[:,:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_width = dataset_depth.shape[1]\n",
    "sensor_height = dataset_depth.shape[0]\n",
    "getCentered3Dindices('x', sensor_width, sensor_height).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(np.mgrid[-hw:hw, -hh:hh], (2, 1, 0))[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_xyz_cut(k, R, t, depth):\n",
    "    assert k[0, 0] == k[1, 1]\n",
    "    focal_length = k[0, 0]\n",
    "    hh = k[1, 2]  # half height\n",
    "    hw = k[0, 2]  # half width\n",
    "    shape = (int(2 * hh), int(2 * hw))\n",
    "\n",
    "    pixel_centers = np.append(np.transpose(np.mgrid[-hw:hw, -hh:hh], (2, 1, 0)) / focal_length, np.ones(shape + (1,)), axis=2)\n",
    "    points = np.matmul(np.tile(R, shape + (1, 1)), pixel_centers[:, :, :, np.newaxis]).squeeze()[:, :, :3]\n",
    "    points = np.multiply(points, np.repeat(depth[:, :, np.newaxis], 3, axis=2))\n",
    "    points = points + t.reshape((1,1,3))\n",
    "    return points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = compute_xyz_cut(calibration_mat, rotation_mat.T, (- rotation_mat.T @ translation), dataset_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('/home/kremeto1/neural_rendering/datasets/post_processed/imc/pantheon_exterior_thesis_test-100_src-fused-splatting/test/0000_depth.png.npy')\n",
    "\n",
    "# plt.hist(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "points = np.matmul(np.tile(R.from_euler('z', 90, degrees=True).as_matrix(), (766, 1025, 1, 1)), points[:, :, :, np.newaxis]).squeeze()[:,:,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XYZ Cut vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose['calibration_mat'][1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import scipy.io as sio\n",
    "import cv2\n",
    "\n",
    "def create_coordinate_frame(R, t, length=1):\n",
    "    # Extract rotation matrix and translation vector from the camera matrix\n",
    "    matrix = np.eye(4)\n",
    "    matrix[:3, :3] = R\n",
    "    matrix[:3, 3] = t\n",
    "\n",
    "    # Define the coordinate frame origin\n",
    "    origin = np.zeros(3)\n",
    "\n",
    "    # Create the coordinate frame using create_mesh_coordinate_frame\n",
    "    coordinate_frame = o3d.geometry.TriangleMesh.create_coordinate_frame(size=length, origin=origin)\n",
    "    \n",
    "    # Transform the coordinate frame to the desired pose\n",
    "    coordinate_frame.transform(matrix)\n",
    "\n",
    "    return coordinate_frame\n",
    "\n",
    "def create_view_frustum_mesh(K, R, t):\n",
    "    \"\"\"\n",
    "    Create a view frustum mesh based on camera intrinsic parameters.\n",
    "\n",
    "    Args:\n",
    "        intrinsics (open3d.camera.PinholeCameraIntrinsic): Camera intrinsic parameters.\n",
    "        depth_range (tuple): Range of depth values to define the near and far clipping planes.\n",
    "\n",
    "    Returns:\n",
    "        open3d.geometry.TriangleMesh: The view frustum mesh.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the corners of the view frustum in camera space\n",
    "\n",
    "    width, height, fx, fy, cx, cy = int(K[0,2]*2), int(K[1,2]*2), K[0,0], K[1,1], K[0,2], K[1,2]\n",
    "\n",
    "    def v(x, y, widht, height, f):\n",
    "        return [\n",
    "            10 * (x * 0.5 * width) / f,\n",
    "            10 * (y * 0.5 * height) / f,\n",
    "            10\n",
    "        ]\n",
    "\n",
    "    corners_camera = np.array([\n",
    "        [0, 0, 0],\n",
    "        v(-1, -1, width, height, fx),\n",
    "        v(1, -1, width, height, fx),\n",
    "        v(1, 1, width, height, fx),\n",
    "        v(-1, 1, width, height, fx),\n",
    "    ])\n",
    "\n",
    "    # Transform the frustum corners to world coordinates\n",
    "    corners_world = (R @ corners_camera.T + t.T).T\n",
    "\n",
    "    # Define the frustum triangles (connecting the corners)\n",
    "    triangles = [\n",
    "        [0, 2, 1],\n",
    "        [0, 3, 2],\n",
    "        [0, 4, 3],\n",
    "        [0, 1, 4]\n",
    "    ]\n",
    "\n",
    "    colors = np.array([[0.3, 0.3, 0.3], [0.3, 0.3, 0.3], [0.3, 0.3, 0.3], [0.3, 0.3, 0.3], [0.3, 0.3, 0.3]])\n",
    "\n",
    "    # Create the view frustum mesh\n",
    "    view_frustum_mesh = o3d.geometry.TriangleMesh()\n",
    "    view_frustum_mesh.vertices = o3d.utility.Vector3dVector(corners_world)\n",
    "    view_frustum_mesh.triangles = o3d.utility.Vector3iVector(triangles)\n",
    "    view_frustum_mesh.vertex_colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "    return view_frustum_mesh\n",
    "\n",
    "matfile_path = '/home/kremeto1/neural_rendering/datasets/final/imc/hagia_sophia_interior_thesis_spheres_squarify_corrected_missing_points-100_src-fused_inloc_format2/matfiles/cutout_0438.png.mat'\n",
    "# matfile_path = '/home/kremeto1/neural_rendering/datasets/final/imc/pantheon_exterior_thesis_spheres_squarify-100_src-fused_inloc_format/matfiles/cutout_0081.png.mat'\n",
    "# matfile_path = '/home/kremeto1/neural_rendering/datasets/final/imc/grand_place_brussels_thesis_spheres_squarify-100_src-fused_inloc_format/matfiles/cutout_0076.png.mat'\n",
    "# matfile_path = '/nfs/projects/artwin/experiments/thesis/artwin_as_inloc/joined-dataset-spheres-inloc_format/matfiles/cutout_0088.png.mat'\n",
    "# matfile_path = '/home/kremeto1/neural_rendering/datasets/final/inloc/inloc_rendered_splatting-inloc_format/matfiles/cutout_DUC_cutout_037_150_0.png.mat'\n",
    "# matfile_path = '/home/kremeto1/neural_rendering/datasets/raw/inloc/database/cutouts/DUC2/037/DUC_cutout_037_150_0.jpg.mat'\n",
    "pose_path = matfile_path.replace('matfiles', 'poses')\n",
    "\n",
    "pose = sio.loadmat(pose_path)\n",
    "frame = create_coordinate_frame(pose['R'], pose['position'])\n",
    "\n",
    "# intrinsics = o3d.camera.PinholeCameraIntrinsic(int(pose['calibration_mat'][0,2]*2), int(pose['calibration_mat'][1,2]*2), pose['calibration_mat'][0,0], pose['calibration_mat'][1,1], pose['calibration_mat'][0,2],pose['calibration_mat'][1,2])\n",
    "\n",
    "# Create the camera view frustum\n",
    "view_frustum = create_view_frustum_mesh(pose['calibration_mat'], pose['R'], pose['position'])\n",
    "\n",
    "\n",
    "matfile = sio.loadmat(matfile_path)\n",
    "print('__header__: ', matfile['__header__'])\n",
    "print('__version__: ', matfile['__version__'])\n",
    "print('__globals__: ', matfile['__globals__'])\n",
    "print('Remaining keys: ', list(matfile.keys())[-2:])\n",
    "points = matfile[\"XYZcut\"]\n",
    "\n",
    "row_linspace = np.linspace(0, 255, points.shape[1], dtype=np.uint8)\n",
    "tiled_per_columns = np.tile(row_linspace, (points.shape[0], 1))\n",
    "colored_columns = cv2.applyColorMap(tiled_per_columns, cv2.COLORMAP_JET) / 255\n",
    "\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points.reshape((-1,3)))\n",
    "pcd.colors = o3d.utility.Vector3dVector(colored_columns.reshape((-1,3)))\n",
    "\n",
    "print('Saving pointcloud to PLY file')\n",
    "o3d.io.write_point_cloud('test.ply', pcd)\n",
    "print('Saving frame to PLY file')\n",
    "o3d.io.write_triangle_mesh('test-frame.ply', frame)\n",
    "print('Saving frustum to PLY file')\n",
    "o3d.io.write_triangle_mesh('test-frustum.ply', view_frustum)\n",
    "\n",
    "# Create a MeshLab server instance\n",
    "# ms = ml.MeshSet()\n",
    "\n",
    "# # Import the PointCloud\n",
    "# ms.load_new_mesh(\"point_cloud\", pcd)\n",
    "\n",
    "# # Import the TriangleMesh\n",
    "# ms.load_new_mesh(\"coord_frame\", frame)\n",
    "\n",
    "# # Merge the meshes\n",
    "# ms.current_mesh().merge(ms.named_mesh(\"coord_frame\"))\n",
    "\n",
    "# # Export the merged mesh as a PLY file\n",
    "# ms.save_current_mesh(\"test.ply\")\n",
    "\n",
    "# # Clean up the MeshLab server\n",
    "# ms.clear()\n",
    "\n",
    "# mmesh = o3d.geometry.TriangleMesh.create_sphere(radius=1.0, resolution=20)\n",
    "# mm = np.eye(4)\n",
    "# mm[:3,3] = (- rotation_mat.T @ translation).squeeze()\n",
    "# mmesh.transform(mm)\n",
    "\n",
    "# o3d.io.write_triangle_mesh('test.ply', mmesh)\n",
    "\n",
    "# from open3d import JVisualizer\n",
    "# visualizer = JVisualizer()\n",
    "# visualizer.add_geometry(pcd)\n",
    "# visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = np.load('/home/kremeto1/neural_rendering/datasets/final/inloc/inloc_rendered_spheres-inloc_format/depthmaps/depth_DUC_cutout_037_150_0.png.npy')\n",
    "# depth[np.abs(depth - 1) < .0005] = 0.0\n",
    "np.abs(depth - 1) < .0005\n",
    "(np.abs(depth - 1) < .5).flatten().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points[points.sum(axis=2)<0.005] = np.nan\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.hist(np.load('/home/kremeto1/neural_rendering/datasets/processed/inloc/inloc_rendered_pyrender/CSE3/000/cse_cutout_000_0_-30_depth.npy'), range=(-1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.load('/home/kremeto1/neural_rendering/datasets/post_processed/imc/pantheon_exterior_thesis_test-100_src-fused-splatting/val/0050_depth.png.npy')[0,0])\n",
    "print(np.load('/home/kremeto1/neural_rendering/datasets/post_processed/imc/pantheon_exterior_thesis_test-100_src-fused-spheres/val/0050_depth.png.npy')[0,0])\n",
    "print(np.load('/home/kremeto1/neural_rendering/datasets/post_processed/imc/pantheon_exterior_thesis_test_squarify-100_src-fused/val/0050_depth.npy')[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(points.reshape((-1,3)))\n",
    "pcd.colors = o3d.utility.Vector3dVector(\n",
    "    ((np.clip(cv2.applyColorMap((np.tile(np.arange(points.shape[1]),(points.shape[0],1))*255/points.shape[1]).astype(np.uint8), cv2.COLORMAP_JET),0,255))/255).reshape((-1,3))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mm = rotation_mat.T.copy()\n",
    "# mm[:, 1:3] *= -1\n",
    "# cc = XYZcut.copy()\n",
    "# tt = (- rotation_mat.T @ translation)\n",
    "# cc[:,:,0] -= tt[0,0]\n",
    "# cc[:,:,1] -= tt[1,0]\n",
    "# cc[:,:,2] -= tt[2,0]\n",
    "# cc = np.square(cc)\n",
    "# cc = np.sum(cc, axis=2)\n",
    "# cc = np.sqrt(cc)\n",
    "dd = np.linalg.norm(points - (- rotation_mat.T @ translation).reshape((1,1,3)), axis=2)\n",
    "# dd=cc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.eye(4)\n",
    "x[:3,:3] = rotation_mat.T\n",
    "x[:, 1:3] *= -1\n",
    "print(x.flatten())\n",
    "print((rotation_mat.T @ translation).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = calibration_mat\n",
    "# hh = k[1, 2]\n",
    "# hw = k[0, 2]\n",
    "# assert k[0, 0] == k[1, 1], \"Camera pixel is not square.\"\n",
    "# focal_length = k[0, 0]\n",
    "\n",
    "# camera_pose = np.eye(4)\n",
    "# camera_pose[:3, :3] = rotation_mat.T\n",
    "# camera_pose[:3, -1:] = (- rotation_mat.T @ translation)\n",
    "# camera_pose[:, 1:3] *= -1\n",
    "\n",
    "# pixel_centers_in_screen_space = np.transpose(np.mgrid[-hh:hh, -hw:hw], (1, 2, 0))\n",
    "# points_in_view_space = np.append(pixel_centers_in_screen_space / focal_length, np.ones(depth.shape + (1,)), axis=2) * np.repeat(depth.reshape(depth.shape + (1,)), 3, axis=2)\n",
    "# points_in_view_space = points_in_view_space.reshape((-1, 3))\n",
    "# points_in_world_space = (camera_pose @ np.concatenate([points_in_view_space, np.ones((points_in_view_space.shape[0], 1))], axis=1).T).T\n",
    "# points_in_world_space = points_in_world_space[:, :3]\n",
    "# points_in_world_space = points_in_world_space.reshape(pixel_centers_in_screen_space.shape[:2] + (3,))\n",
    "# points_in_world_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = sio.loadmat('/home/kremeto1/neural_rendering/datasets/final/inloc/inloc_rendered_pyrender_inloc_format/matfiles/cutout_cse_cutout_000_0_-30.png.mat')[\"XYZcut\"]\n",
    "poses = sio.loadmat('/home/kremeto1/neural_rendering/datasets/final/inloc/inloc_rendered_pyrender_inloc_format/poses/cutout_cse_cutout_000_0_-30.png.mat')\n",
    "prj = plt.imread('/home/kremeto1/neural_rendering/datasets/final/inloc/inloc_rendered_pyrender_inloc_format/cutouts/cutout_cse_cutout_000_0_-30.png', cv2.IMREAD_UNCHANGED)[:, :, :3]\n",
    "rotation_mat = poses['R']\n",
    "translation = poses['position'].T\n",
    "calibration_mat = poses['calibration_mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = calibration_mat\n",
    "hh = k[1, 2]\n",
    "hw = k[0, 2]\n",
    "assert k[0, 0] == k[1, 1], \"Camera pixel is not square.\"\n",
    "focal_length = k[0, 0]\n",
    "\n",
    "# pts = np.load('/nfs/projects/artwin/experiments/thesis/artwin_as_inloc/joined-dataset-pyrender-inloc_format/matfiles/cutout_0075.png.mat.npy').reshape((-1, 3))\n",
    "# pts = points_in_world_space.copy().reshape((-1, 3))\n",
    "# pts[:, 1] *= -1\n",
    "scene = pyrender.Scene(bg_color=[0,0,0])\n",
    "mesh = pyrender.Mesh.from_points(points.reshape(-1,3))\n",
    "# mesh = load_ply('test.ply', None)\n",
    "\n",
    "scene.add(mesh)\n",
    "camera = pyrender.camera.IntrinsicsCamera(\n",
    "    k[0, 0], k[1, 1], k[0, 2], k[1, 2]\n",
    ")\n",
    "cam_pose = np.eye(4)\n",
    "cam_pose[:3, :3] = rotation_mat.T\n",
    "cam_pose[:3, -1:] = (- rotation_mat.T @ translation)\n",
    "# cam_pose[:, 1:3] *= -1\n",
    "# cam_pose[:3, :3] = np.array([[0,-1,0],[1,0,0],[0,0,1]]) @ cam_pose[:3, :3]\n",
    "# cam_pose[:3, -1:] = translation\n",
    "# cam_pose[:3, :3] = np.array([[0, 1, 0], [1, 0, 0], [0, 0, 1]], dtype=np.float) @ cam_pose[:3, :3]\n",
    "scene.add(camera, pose=cam_pose)\n",
    "\n",
    "# Offscreen rendering\n",
    "r = pyrender.OffscreenRenderer(hw*2, hh*2, point_size=3)\n",
    "rgb_rendering, depth_rendering = r.render(scene, flags=pyrender.RenderFlags.FLAT)\n",
    "r.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "# plt.imshow((np.clip(squarify(cv2.applyColorMap((255*depth_rendering/20).astype(np.uint8), cv2.COLORMAP_HSV), 1248),0,255) * 0.5 + 0.5 * prj).astype(np.uint8))\n",
    "plt.imshow((np.clip(cv2.applyColorMap((255*depth_rendering/20).astype(np.uint8), cv2.COLORMAP_HSV),0,255) * 0.5 + 0.5 * prj).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "# plt.imshow(np.divide(dd, np.sqrt(np.square(np.transpose(np.mgrid[-hh:hh, -hw:hw], (1, 2, 0)) / focal_length).sum(axis=2) + 1)))\n",
    "plt.imshow((np.clip(cv2.applyColorMap((np.tile(np.arange(1025),(766,1))*255/1025).astype(np.uint8), cv2.COLORMAP_HSV),0,255)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(prj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = sio.loadmat('/home/kremeto1/inloc/datasets/hagia-pyrender/densePE_top100_shortlist.mat')\n",
    "index = 0\n",
    "query_path = Path(str(mat['ImgList'][0][index][0][0]))\n",
    "candidate_paths = [Path(str(i[0])) for i in mat['ImgList'][0][index][1][0]]\n",
    "# candidate_scores = mat['ImgList'][0][index][2][0]\n",
    "candidate_transforms = [i for i in mat['ImgList'][0][index][3][0]]\n",
    "candidate_r = [i for i in mat['ImgList'][0][index][4][0]]\n",
    "candidate_t = [i for i in mat['ImgList'][0][index][5][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load('/home/kremeto1/neural_rendering/datasets/processed/inloc/inloc_rendered_spheres/DUC1/088/DUC_cutout_088_210_30_depth.png.npy')\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(cv2.resize(cv2.rotate(plt.imread('/home/kremeto1/neural_rendering/datasets/raw/inloc/query/iphone7/IMG_0731.JPG'), cv2.ROTATE_90_CLOCKWISE), (1600,1200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Path('/home/kremeto1/neural_rendering/datasets/raw/inloc/query/iphone7/').glob('*.JPG'):\n",
    "    plt.imsave(str(i).replace('iphone7', 'same_db_size'), cv2.resize(plt.imread(str(i)), (1600,1200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Path('/home/kremeto1/neural_rendering/datasets/raw/inloc/query/iphone7/').glob('*.JPG'):\n",
    "    new_path = str(i).replace('iphone7', 'same_db_size')\n",
    "    image = plt.imread(str(i))\n",
    "    if image.shape[:2] == (4032, 3024):\n",
    "        image = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n",
    "    new_img = cv2.resize(image, (1600, int(3024 * 1600 / 4032)))\n",
    "    # img = np.zeros((1200, 1600, 3), dtype=image.dtype)\n",
    "    # h, w = new_img.shape[:2]\n",
    "    # # offset_h = (1600 - h) // 2\n",
    "    # offset_w = (1600 - w) // 2\n",
    "    # img[:, offset_w : offset_w + w, ...] = new_img\n",
    "    plt.imsave(new_path, new_img)\n",
    "    print(i.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking depths of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import skimage\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, sample_size=None):\n",
    "    depth_maps = []\n",
    "    for part in [\"train\", \"val\"]:\n",
    "        depths = list((Path(dataset) / part).glob(\"*_depth.png\"))\n",
    "        random.shuffle(depths)\n",
    "        for depth in depths[:sample_size]:\n",
    "            # read as uint16\n",
    "            depth = skimage.io.imread(str(depth))\n",
    "            depth_maps.append(depth.flatten())\n",
    "    return np.concatenate(depth_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset_npy(dataset, sample_size=None):\n",
    "    depth_maps = []\n",
    "    depths = list(Path(dataset).glob(\"**/*.npy\"))\n",
    "    random.shuffle(depths)\n",
    "    for depth in depths[:sample_size]:\n",
    "        # read as uint16\n",
    "        depth = np.load(str(depth))\n",
    "        depth_maps.append(depth.flatten())\n",
    "    return np.concatenate(depth_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hist(collection, data, labels, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.hist(\n",
    "        data,\n",
    "        label=labels,\n",
    "        log=True,\n",
    "        **kwargs\n",
    "    )\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.set_xlabel('Depths')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_title(f'Histogram of depths of {collection}')\n",
    "    #plt.savefig(f'{collection}_hist.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyrender = process_dataset('/home/kremeto1/neural_rendering/datasets/post_processed/inloc/inloc_rendered_pyrender', 250)\n",
    "spheres = process_dataset('/home/kremeto1/neural_rendering/datasets/post_processed/inloc/inloc_rendered_spheres', 250)\n",
    "splatting = process_dataset('/home/kremeto1/neural_rendering/datasets/post_processed/inloc/inloc_rendered_splatting', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hist(collection=\"inloc\", data=[pyrender]+[spheres]+[splatting], labels=['pyrender', 'spheres', 'splatting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyrender_npy = process_dataset_npy('/home/kremeto1/neural_rendering/datasets/processed/inloc/inloc_rendered_pyrender', 250)\n",
    "spheres_npy = process_dataset_npy('/home/kremeto1/neural_rendering/datasets/processed/inloc/inloc_rendered_spheres', 250)\n",
    "splatting_npy = process_dataset_npy('/home/kremeto1/neural_rendering/datasets/processed/inloc/inloc_rendered_splatting', 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hist(collection=\"inloc\", data=[pyrender_npy]+[spheres_npy]+[splatting_npy], labels=['pyrender', 'spheres', 'splatting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/kremeto1/neural_rendering/datasets/processed/imc/hagia_sophia_interior/dense/dense/0/fused_25.ply.kdtree.radii') as f:\n",
    "    radii25 = [float(i) for i in f.readlines()[0].split(' ')[5:]]\n",
    "with open('/home/kremeto1/neural_rendering/datasets/processed/imc/hagia_sophia_interior/dense/dense/0/fused_50.ply.kdtree.radii') as f:\n",
    "    radii50 = [float(i) for i in f.readlines()[0].split(' ')[5:]]\n",
    "with open('/home/kremeto1/neural_rendering/datasets/processed/imc/hagia_sophia_interior/dense/dense/0/fused.ply.kdtree.radii') as f:\n",
    "    radii = [float(i) for i in f.readlines()[0].split(' ')[5:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 1, 100)[1:-1], np.quantile(radii25, np.linspace(0, 1, 100)[1:-1]))\n",
    "plt.plot(np.linspace(0, 1, 100)[1:-1], np.quantile(radii50, np.linspace(0, 1, 100)[1:-1]))\n",
    "plt.plot(np.linspace(0, 1, 100)[1:-1], np.quantile(radii, np.linspace(0, 1, 100)[1:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.quantile(radii25, 0.9))\n",
    "print(np.quantile(radii50, 0.9))\n",
    "print(np.quantile(radii, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_hist(collection=\"radii\", data=[radii25]+[radii50]+[radii], labels=['hagia25', 'hagia50', 'hagia'], bins=100)\n",
    "\n",
    "my_hist(collection=\"radii\", data=[radii25], labels=['hagia25'])\n",
    "my_hist(collection=\"radii\", data=[radii50], labels=['hagia50'], bins=100)\n",
    "my_hist(collection=\"radii\", data=[radii], labels=['hagia'], bins=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
